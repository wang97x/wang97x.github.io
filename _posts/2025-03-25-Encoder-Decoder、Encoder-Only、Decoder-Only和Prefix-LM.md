---
layout: post
title: "Encoder-Decoder、Encoder-Only、Decoder-Only和Prefix-LM"
author: "wang"
date: 2025-03-25 20:20:00 +0800
categories: [LLM]
tags: [llm]
---
以下是四种主要模型架构的详细中文解析：

---

### 1. **编码器-解码器架构（Encoder-Decoder）**
   - **结构**：包含编码器（处理输入）和解码器（生成输出）。
   - **典型任务**：序列到序列（seq2seq）任务，如机器翻译、文本摘要。
   - **代表模型**：T5、BART、原始Transformer。
   - **工作原理**：
     - **编码器**：通过双向注意力（能看到全部上下文）将输入转换为稠密的语义表示（如向量）。
     - **解码器**：基于编码器的输出和已生成的历史词，以自回归方式（逐个生成）预测下一个词。
   - **特点**：适合输入和输出长度可变的任务，但训练需同时优化编码器和解码器。

---

### 2. **纯编码器架构（Encoder-Only）**
   - **结构**：仅保留编码器，无解码器。
   - **典型任务**：需要深度理解输入的任务，如文本分类、命名实体识别（NER）、情感分析。
   - **代表模型**：BERT、RoBERTa。
   - **工作原理**：
     - 预训练目标：通常为掩码语言建模（Masked Language Modeling，MLM），随机遮盖输入词并预测。
     - 微调：输出输入序列的上下文表示（如[CLS]标签），用于分类或序列标注。
   - **特点**：无法直接生成文本，但擅长提取语义特征。

---

### 3. **纯解码器架构（Decoder-Only）**
   - **结构**：仅保留解码器，无编码器。
   - **典型任务**：自回归文本生成，如故事创作、代码补全、问答。
   - **代表模型**：GPT系列、LLaMA、PaLM。
   - **工作原理**：
     - 输入和输出合并为单一序列，通过因果注意力（Causal Attention）确保生成时仅能看到左侧上下文（避免信息泄露）。
     - 自回归生成：逐词预测，每一步依赖已生成的词。
   - **特点**：生成能力强，但对长输入的处理可能受限（需缓存历史状态）。

---

### 4. **前缀语言模型（Prefix-LM）**
   - **结构**：解码器的变体，支持部分双向注意力。
   - **典型任务**：需在固定前缀（如提示）基础上生成的任务，例如对话、受限文本生成。
   - **代表模型**：UniLM、ChatGLM。
   - **工作原理**：
     - **前缀部分**：允许双向注意力，像编码器一样充分理解输入前缀（如用户的问题）。
     - **生成部分**：切换到因果注意力，像解码器一样自回归生成后续内容。
   - **特点**：结合编码器和解码器的优势，灵活处理“理解+生成”混合任务。

---

### **核心对比表**
| **架构**          | **核心组件**       | **注意力机制**                     | **典型应用**                     |
|--------------------|--------------------|-----------------------------------|----------------------------------|
| 编码器-解码器      | 编码器 + 解码器    | 编码器：双向 <br> 解码器：因果     | 翻译、摘要                       |
| 纯编码器           | 仅编码器           | 双向（全局上下文）                | 分类、NER、问答匹配              |
| 纯解码器           | 仅解码器           | 因果（仅左侧上下文）              | 创作、补全、开放式生成           |
| 前缀语言模型       | 改进的解码器       | 前缀：双向 <br> 生成：因果         | 对话、提示引导生成               |

---

### **总结：如何选择架构？**
- **需要同时理解输入并生成输出** ➜ **编码器-解码器**（如翻译）。
- **仅需理解输入，无需生成** ➜ **纯编码器**（如情感分析）。
- **开放式生成任务** ➜ **纯解码器**（如GPT写文章）。
- **输入包含固定前缀+生成需求** ➜ **前缀语言模型**（如ChatGPT对话）。

---

### **附加说明**
- **训练成本**：编码器-解码器通常参数量更大，训练更复杂；纯解码器更易扩展（如GPT-4）。
- **灵活性**：前缀语言模型通过动态划分前缀和生成部分，可适配多种任务，但实现复杂度较高。